{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "553f2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import sys\n",
    "import d2l.torch as d2l\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b329f5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import init\n",
    "batch_size = 256\n",
    "train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "# 初始化模型参数 w 和 b\n",
    "w = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self,num_inputs,num_outputs):\n",
    "        super(LinearNet,self).__init__()\n",
    "        self.linear = nn.Linear(num_inputs,num_outputs)\n",
    "    def forward(self,x):\n",
    "        y = self.linear(x.view(x.shape[0],-1))\n",
    "        return y\n",
    "net = LinearNet(num_inputs,num_outputs)\n",
    "\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "    \n",
    "    def forward(self, x):  # x shape: (batch, *, *, ...)\n",
    "        return x.view(x.shape[0], -1)\n",
    "from collections import OrderedDict\n",
    "net = nn.Sequential(\n",
    " # FlattenLayer(),\n",
    " # nn.Linear(num_inputs, num_outputs)\n",
    " OrderedDict([\n",
    " ('flatten', FlattenLayer()),\n",
    " ('linear', nn.Linear(num_inputs, num_outputs))])\n",
    " )\n",
    "init.normal_(net.linear.weight, mean=0, std=0.01)\n",
    "init.constant_(net.linear.bias, val=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32791545",
   "metadata": {},
   "source": [
    "定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "644d5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ad44f",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e6b1cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0091, train acc 0.136, test acc 0.136\n",
      "epoch 2, loss 0.0091, train acc 0.136, test acc 0.136\n",
      "epoch 3, loss 0.0091, train acc 0.136, test acc 0.136\n",
      "epoch 4, loss 0.0091, train acc 0.136, test acc 0.136\n",
      "epoch 5, loss 0.0091, train acc 0.136, test acc 0.136\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "# 辅助函数\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "num_epochs,lr = 5, 0.1\n",
    "def train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,params=None,lr=None,optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X,y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat,y).sum()\n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward()\n",
    "            if optimizer is None:\n",
    "                sgd(params,lr,batch_size)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'% (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "# 修复后的SGD函数：增加对grad是否为None的判断\n",
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"小批量随机梯度下降算法\"\"\"\n",
    "    if lr is None or batch_size is None:\n",
    "        raise ValueError(\"学习率(lr)和批量大小(batch_size)不能为None\")\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            if param.grad is not None:\n",
    "                param -= lr * param.grad / batch_size\n",
    "\n",
    "# 确保以下变量已正确定义：\n",
    "# net, train_iter, test_iter, w, b, batch_size\n",
    "\n",
    "# 调用训练函数\n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, [w, b], lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
